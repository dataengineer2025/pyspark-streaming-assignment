# -*- coding: utf-8 -*-
"""final_assignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xX8iur0gW4elPFWIvVR6JJJ0xMXS2tdq
"""



"""here the actual code starts"""

# Commented out IPython magic to ensure Python compatibility.
from IPython import get_ipython
from IPython.display import display
# %%
!pip install pyspark boto3
# %%
from pyspark.sql import SparkSession
import os # Import os for environment variables


spark = SparkSession.builder \
    .appName("MechanismX_Assignment") \
    .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.2.0,com.amazonaws:aws-java-sdk-bundle:1.11.375") \
    .getOrCreate()

spark
# %% [markdown]
#
# %%
!git clone https://github.com/dataengineer2025/pyspark-streaming-assignment.git

# %%
# %cd pyspark-streaming-assignment
# %%
from google.colab import files

uploaded = files.upload()
# %%
!ls
# %%

# %%
# prompt: how to know my files are in which directory help me the Spark DataFrames are read from the original `/content/` path:

# Read input files
df_txn = spark.read.csv("/content/pyspark-streaming-assignment/transactions.csv", header=True, inferSchema=True)
df_cust = spark.read.csv("/content/pyspark-streaming-assignment/CustomerImportance.csv", header=True, inferSchema=True)

df_txn.show(3)
df_cust.show(3)

# %%
total_records = df_txn.count()
print(f"Total transactions: {total_records}")

# %%
!pip install boto3
import boto3
import os

# %%
# Ensure AWS credentials are set as environment variables for Spark to pick up
# when reading from s3a://
os.environ["AWS_ACCESS_KEY_ID"] = ""
os.environ["AWS_SECRET_ACCESS_KEY"] = ""
# Change the region name to the correct format 'eu-north-1'
os.environ["AWS_DEFAULT_REGION"] = "eu-north-1"
# %%
s3 = boto3.client("s3")
bucket_name = "assignment-sumi-bucket"
# %%
from pyspark.sql import Row
import time
import uuid
import shutil
import os

chunk_size = 10000
total_rows = df_txn.count()
num_chunks = (total_rows + chunk_size - 1) // chunk_size

print(f"📦 Total Chunks to Process: {num_chunks}")

schema = df_txn.schema

rdd_indexed = df_txn.rdd.zipWithIndex().cache()

for i in range(num_chunks):
    start_idx = i * chunk_size
    end_idx = start_idx + chunk_size

    print(f"\n🟩 Processing chunk {i + 1} — Rows {start_idx} to {min(end_idx, total_rows)}")

    # Filter RDD rows between index range
    chunk_rdd = rdd_indexed.filter(lambda row: start_idx <= row[1] < end_idx).map(lambda row: row[0])
    chunk_df = spark.createDataFrame(chunk_rdd, schema)

    # Create local folder
    local_tmp_dir = f"/content/tmp_chunk_{uuid.uuid4().hex}"
    os.makedirs(local_tmp_dir, exist_ok=True)

    # Save chunk to local CSV
    chunk_df.coalesce(1).write.mode("overwrite").option("header", True).csv(local_tmp_dir)

    # Find the actual .csv file (not _SUCCESS or _metadata)
    file_name = [f for f in os.listdir(local_tmp_dir) if f.endswith(".csv") and not f.startswith("_")][0]
    local_file_path = os.path.join(local_tmp_dir, file_name)

    # Define S3 path for upload using boto3
    s3_key = f"chunks/txn_chunk_{i + 1}.csv"
    s3.upload_file(local_file_path, bucket_name, s3_key)

    print(f"✅ Uploaded chunk {i + 1} to S3: s3://{bucket_name}/{s3_key}")

    # Cleanup local temporary directory
    shutil.rmtree(local_tmp_dir)
    time.sleep(1) # Add a small delay



"""PatId1 detection and uploads"""

from pyspark.sql.functions import col, count, percent_rank, current_timestamp, lit
from pyspark.sql.window import Window
import boto3, os, uuid, shutil, time
from pyspark.sql import SparkSession

# ==========================
# ✅ Spark & AWS Setup
# ==========================
os.environ["AWS_ACCESS_KEY_ID"] = "YOUR_ACCESS_KEY_ID"
os.environ["AWS_SECRET_ACCESS_KEY"] = "YOUR_SECRET_ACCESS_KEY"
os.environ["AWS_DEFAULT_REGION"] = "eu-north-1"

spark = SparkSession.builder \
    .appName("MechanismY_PatId1") \
    .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.2.0,com.amazonaws:aws-java-sdk-bundle:1.11.375") \
    .getOrCreate()

# ==========================
# ✅ Constants
# ==========================
bucket_name = "assignment-sumi-bucket"
chunk_prefix = "chunks/"
s3 = boto3.client("s3")

# ==========================
# ✅ Load Enrichment File (CustomerImportance)
# ==========================
df_cust = spark.read.csv("/content/pyspark-streaming-assignment/CustomerImportance.csv", header=True, inferSchema=True)

# Use correct customer ID column name
customer_id_col = "Source" if "Source" in df_cust.columns else "customerId"

# Bottom 1% weight
w_weight = Window.orderBy("weight")
df_with_weight_rank = df_cust.withColumn("weight_rank", percent_rank().over(w_weight))
bottom_1_weight = df_with_weight_rank.filter(col("weight_rank") <= 0.01).select(customer_id_col).distinct()

# ==========================
# ✅ Process Each Chunk from S3
# ==========================
response = s3.list_objects_v2(Bucket=bucket_name, Prefix=chunk_prefix)
chunk_keys = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.csv')]
print(f"\n📂 Total chunks found: {len(chunk_keys)}")

for i, key in enumerate(chunk_keys):
    print(f"\n🔄 Processing chunk {i + 1}: {key}")

    # ✅ Download from S3 to local
    local_chunk_path = f"/content/{os.path.basename(key)}"
    s3.download_file(bucket_name, key, local_chunk_path)

    # ✅ Read locally into Spark
    df_chunk = spark.read.csv(local_chunk_path, header=True, inferSchema=True)

    # ✅ Join enrichment (explicit join if column names differ)
    df_joined = df_chunk.join(df_cust, df_chunk["customer"] == df_cust[customer_id_col], how="left")

    # ✅ Merchants with > 50k transactions
    df_merchant_counts = df_joined.groupBy("merchant").agg(count("*").alias("txn_count"))
    df_big_merchants = df_merchant_counts.filter(col("txn_count") > 50000)
    df_filtered = df_joined.join(df_big_merchants, on="merchant", how="inner")

    # ✅ Top 1% by step per merchant (step replaces transactionId)
    w_txn = Window.partitionBy("merchant").orderBy("step")
    df_with_rank = df_filtered.withColumn("txn_rank", percent_rank().over(w_txn))
    top_1_percent = df_with_rank.filter(col("txn_rank") <= 0.01)

    # ✅ Join top 1% customers with bottom 1% weight
    df_upgrade = top_1_percent.join(bottom_1_weight, top_1_percent["customer"] == bottom_1_weight[customer_id_col], how="inner")

    # ✅ Select output
    result = df_upgrade.select(
        current_timestamp().alias("YStartTime"),
        current_timestamp().alias("detectionTime"),
        lit("PatId1").alias("patternId"),
        lit("Upgrade").alias("ActionType"),
        col("customer").alias("customerName"),
        col("merchant").alias("merchantId")
    )

    result_count = result.count()
    print(f"📦 Found {result_count} records in chunk {i + 1}")

    if result_count == 0:
        continue

    # ✅ zipWithIndex for row_id
    indexed_rdd = result.rdd.zipWithIndex().map(lambda row_index: (*row_index[0], row_index[1]))
    result_with_id = spark.createDataFrame(indexed_rdd, result.schema.add("row_id", "long"))

    batch_size = 50
    batch_count = (result_count + batch_size - 1) // batch_size

    for b in range(batch_count):
        start_idx = b * batch_size
        end_idx = start_idx + batch_size

        result_batch = result_with_id.filter((col("row_id") >= start_idx) & (col("row_id") < end_idx)).drop("row_id")

        tmp_dir = f"/content/tmp_result_{uuid.uuid4().hex}"
        os.makedirs(tmp_dir, exist_ok=True)
        result_batch.coalesce(1).write.mode("overwrite").option("header", True).csv(tmp_dir)

        csv_files = [f for f in os.listdir(tmp_dir) if f.endswith(".csv") and not f.startswith("_")]
        if csv_files:
            local_file = os.path.join(tmp_dir, csv_files[0])
            s3_key = f"patterns/patid1_chunk{i+1}_batch{b+1}.csv"
            s3.upload_file(local_file, bucket_name, s3_key)
            print(f"✅ Uploaded: s3://{bucket_name}/{s3_key}")
        else:
            print(f"⚠️ No CSV file created for chunk {i + 1}, batch {b + 1}")

        shutil.rmtree(tmp_dir)
        time.sleep(0.1)

print("\n✅ Completed PatId1 detection and uploads.")



"""pattern 2"""

from pyspark.sql.functions import col, sum as _sum, current_timestamp, lit
from pyspark.sql import Window

pattern_id = "PatId2"
action_type = "Child"

for i, key in enumerate(chunk_keys):
    print(f"\n🔄 Processing chunk {i + 1}: {key}")

    # Download chunk
    local_chunk_path = f"/content/{os.path.basename(key)}"
    s3.download_file(bucket_name, key, local_chunk_path)

    # Read locally
    df_chunk = spark.read.csv(local_chunk_path, header=True, inferSchema=True)

    # Group by customer and sum transaction amounts
    df_grouped = df_chunk.groupBy("customer").agg(_sum("amount").alias("total_amount"))

    # Filter for customers with total amount < 1000
    df_child = df_grouped.filter(col("total_amount") < 1000)

    # Join back to get merchant info
    df_final = df_child.join(df_chunk, on="customer", how="inner").select(
        current_timestamp().alias("YStartTime"),
        current_timestamp().alias("detectionTime"),
        lit(pattern_id).alias("patternId"),
        lit(action_type).alias("ActionType"),
        col("customer").alias("customerName"),
        col("merchant").alias("merchantId")
    ).distinct()

    result_count = df_final.count()
    print(f"📦 Found {result_count} CHILD records in chunk {i + 1}")

    if result_count == 0:
        continue

    # zipWithIndex for batching
    indexed_rdd = df_final.rdd.zipWithIndex().map(lambda row_index: (*row_index[0], row_index[1]))
    df_with_id = spark.createDataFrame(indexed_rdd, df_final.schema.add("row_id", "long"))

    batch_size = 50
    batch_count = (result_count + batch_size - 1) // batch_size

    for b in range(batch_count):
        start_idx = b * batch_size
        end_idx = start_idx + batch_size

        batch = df_with_id.filter((col("row_id") >= start_idx) & (col("row_id") < end_idx)).drop("row_id")

        tmp_dir = f"/content/tmp_patid2_{uuid.uuid4().hex}"
        os.makedirs(tmp_dir, exist_ok=True)
        batch.coalesce(1).write.mode("overwrite").option("header", True).csv(tmp_dir)

        csv_files = [f for f in os.listdir(tmp_dir) if f.endswith(".csv") and not f.startswith("_")]
        if csv_files:
            local_file = os.path.join(tmp_dir, csv_files[0])
            s3_key = f"patterns/patid2_chunk{i+1}_batch{b+1}.csv"
            s3.upload_file(local_file, bucket_name, s3_key)
            print(f"✅ Uploaded: s3://{bucket_name}/{s3_key}")
        else:
            print(f"⚠️ No file for chunk {i+1}, batch {b+1}")

        shutil.rmtree(tmp_dir)
        time.sleep(0.1)

print("\n✅ Completed PatId2 (CHILD) detection.")

"""PatId3"""

from pyspark.sql.functions import col, current_timestamp, lit, regexp_replace
import boto3, os, uuid, shutil, time

# Pattern constants
pattern_id = "PatId3"
action_type = "DeiNeeded"

for i, key in enumerate(chunk_keys):
    print(f"\n🔄 Processing chunk {i + 1}: {key}")

    # Download chunk locally from S3
    local_chunk_path = f"/content/{os.path.basename(key)}"
    s3.download_file(bucket_name, key, local_chunk_path)

    # Load the chunk into Spark
    df_chunk = spark.read.csv(local_chunk_path, header=True, inferSchema=True)

    # Filter for female customers
    df_female = df_chunk.filter(col("gender") == "'F'")  # gender is also quoted

    # Strip quotes from customer and Source
    df_female = df_female.withColumn("customer_clean", regexp_replace(col("customer"), "'", ""))
    df_cust_cleaned = df_cust.withColumn("Source_clean", regexp_replace(col(customer_id_col), "'", ""))

    # Join on cleaned customer IDs
    df_important_female = df_female.join(
        df_cust_cleaned,
        df_female["customer_clean"] == df_cust_cleaned["Source_clean"],
        how="inner"
    )

    # Prepare result DataFrame
    result = df_important_female.select(
        current_timestamp().alias("YStartTime"),
        current_timestamp().alias("detectionTime"),
        lit(pattern_id).alias("patternId"),
        lit(action_type).alias("ActionType"),
        col("customer_clean").alias("customerName"),
        col("merchant").alias("merchantId")
    ).distinct()

    result_count = result.count()
    print(f"📦 Found {result_count} FEMALE DeiNeeded records in chunk {i + 1}")

    if result_count == 0:
        continue

    # zipWithIndex for batching
    indexed_rdd = result.rdd.zipWithIndex().map(lambda row_index: (*row_index[0], row_index[1]))
    df_with_id = spark.createDataFrame(indexed_rdd, result.schema.add("row_id", "long"))

    # Write results in 50-record batches
    batch_size = 50
    batch_count = (result_count + batch_size - 1) // batch_size

    for b in range(batch_count):
        start_idx = b * batch_size
        end_idx = start_idx + batch_size

        result_batch = df_with_id.filter((col("row_id") >= start_idx) & (col("row_id") < end_idx)).drop("row_id")

        # Save locally
        tmp_dir = f"/content/tmp_patid3_{uuid.uuid4().hex}"
        os.makedirs(tmp_dir, exist_ok=True)
        result_batch.coalesce(1).write.mode("overwrite").option("header", True).csv(tmp_dir)

        # Upload to S3
        csv_files = [f for f in os.listdir(tmp_dir) if f.endswith(".csv") and not f.startswith("_")]
        if csv_files:
            local_file = os.path.join(tmp_dir, csv_files[0])
            s3_key = f"patterns/patid3_chunk{i+1}_batch{b+1}.csv"
            s3.upload_file(local_file, bucket_name, s3_key)
            print(f"✅ Uploaded: s3://{bucket_name}/{s3_key}")
        else:
            print(f"⚠️ No CSV file created for chunk {i+1}, batch {b+1}")

        shutil.rmtree(tmp_dir)
        time.sleep(0.1)

print("\n✅ Completed PatId3 (Female DeiNeeded) detection.")



